<!DOCTYPE html>
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=gbk">
		<link rel="stylesheet" href="./css/jemdoc.css" type="text/css">
		<title>主页</title>
	</head>
	<body>
		<div class="main">
			<div class="menu">
				<div class="menu-item">
					<a class="current">Home</a>
				</div>
			</div>
			<table id="tlayout">
				<tbody>
					<tr>
						<td id="layout-content">
							<div class="toptitle">	
								</div>
							<div class="top-box">
								<div class="cell-box left-box">
									<table class="imgtable">
										<tbody>
<tr>
<td>
<p><h3>网易伏羲虚拟人 <br>   <br>Virtual Human Group<br>Netease Fuxi AI Lab<br></h3><br>
We are looking for algorithm interns (PhD/master students) to work in 2022. <br>
The position requires full-time working for at least 4 months. <br>
Contact email: dingyu01@corp.netease.com<br>
</p>
</td>
											</tr>
										</tbody>
									</table>
								</div>
								<div class="cell-box right-box">
<!-- 									<div class="toptitle"> -->
										<h2>News</h2>
									<!-- </div> -->
									<table class="imgtable">
										<tbody>
											<tr>
												<td>
													<ul>
														
<li><p>[2021-11] One paper accepted to AAAI 2022.</p></li>
<li><p>[2021-09] One paper accepted to TVCG.</p></li>
<li>
<p>[2021-07] Our team is the <b><font color="Purple">WINNER (1st)</font></b> of the EXPR Challenge (Basic Expression Classification) on ICCV 2021.</p>
<p>[2021-07] Our team is the <b><font color="Purple">WINNER (1st)</font></b> of the AU Challenge (facial Action Unit Detection) on ICCV 2021.</p>
<p>[2021-07] Our team is the runner-up (2nd) of the VA Challenge (Valence-arousal estimation) on ICCV 2021.</p>                                                                                                                                                                                                                                                 
</li>
<li><p>[2021-03] Two papers accepted to CVPR.</p></li>
<li><p>[2020-11] One paper accepted to AAAI.</p></li>
<li><p>[2020-03] One paper accepted to ICASSP.</p></li>
<li><p>[2020-03] One paper accepted to CVPR.</p></li>

													</ul>
												</td>
											</tr>
										</tbody>
									</table>
								</div>
							</div>
							<h2>Selected Publications </h2>
							<ul>
								<table class="imgtable">
									<tbody>
<table class="imgtable"><tbody><tr><td>
<img src="./img/2022-AAAI-TalkingFace.jpeg" alt="" width="200px" height="130px">&nbsp;
</td>
<td>
<h3><font color="Purple">One-shot Talking Face Generation with Single-speaker Audio-Visual Correlation Learning</font></h3>
<p>Suzhen Wang, Lincheng Li, Yu Ding*, Xin Yu</p>
<p>AAAI Conference on Artificial Intelligence (AAAI), 2022 <br> <p>
</td></tr></tbody></table>

<table class="imgtable"><tbody><tr><td>
<img src="./img/2021-TVCG-GuzhengAnim.jpg" alt="" width="200px" height="130px">&nbsp;
</td>
<td>
<h3><font color="Purple">A Music-driven Deep Generative Model for Guzheng Playing Animation</font></h3>
<p>Jiali Chen, Changjie Fan, Zhimeng Zhang, Gongzheng Li, Zeng Zhao, Jiajun Bu, Zhigang Deng, Yu Ding*</p>
<p>IEEE Transactions on Visualization and Computer Graphics (TVCG) <br> <p>
<div class="material"> 
[<a href="http://graphics.cs.uh.edu/wp-content/papers/2021/2021-TVCG-music-driven-guzheng-animation.pdf">pdf</a>]
[<a href="https://www.youtube.com/watch?v=-XmGKZGddxY&t=8s">video</a>]
</div>
</td></tr></tbody></table>

<table class="imgtable"><tbody><tr><td>
<img src="./img/2021-IJCAI-TalkingFace.jpeg" alt="" width="200px" height="130px">&nbsp;
</td>
<td>
<h3><font color="Purple">Audio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion</font></h3>
<p>Suzhen Wang, Lincheng Li, Yu Ding*, Changjie Fan, Xin Yu,</p>
<p>International Joint Conference on Artificial Intelligence (IJCAI), 2021.<br> <p>
<div class="material"> 
[<a href="https://www.ijcai.org/proceedings/2021/0152.pdf">pdf</a>]
[<a href="https://www.youtube.com/watch?v=4BetZLPHRKs">video</a>]
</div>
</td></tr></tbody></table>

<table class="imgtable"><tbody><tr><td>
<img src="./img/2021-ICCV.jpg" alt="" width="200px" height="130px">&nbsp;
</td>
<td>
<h3><font color="Purple">Prior Aided Streaming Network for Multi-task Affective Analysis</font></h3>
<p>Wei Zhang, Zunhu Guo, Keyu Chen, Lincheng Li, Zhimeng Zhang, Yu Ding, Runze Wu, Tangjie Lv, Changjie Fan</p>
<p>International Conference on Computer Vision (ICCV), 2021<p>
<div class="material"> 
[<a href="./pdf/ICCV2021W_ABAW2_Competition.pdf">pdf</a>]
</div>
</td></tr></tbody></table>

<table class="imgtable"><tbody><tr><td>
<img src="./img/2021-ICCV-ABAW2.jpeg" alt="" width="200px" height="130px">&nbsp;
</td>
<td>
<h3><font color="Purple">Prior aided streaming network for multi-task affective recognitionat the 2nd abaw2 competition</font></h3>
<p>Wei Zhang, Zunhu Guo, Keyu Chen, Lincheng Li, Zhimeng Zhang, Yu Ding*</p>
<p>ICCV-ABAW2 competitions, 2021 </p>
<p>1st place in both EXPR and AU Challenges and 2nd place in the VA Challenge </p>
<div class="material"> 
[<a href="https://arxiv.org/pdf/2107.03708.pdf">pdf</a>]
</div>
</td></tr></tbody></table>

<table class="imgtable"><tbody><tr><td>
<img src="./img/2021-CAWA.jpg" alt="" width="200px" height="130px">&nbsp;
</td>
<td>
<h3><font color="Purple">Learning a deep motion interpolation network for human skeleton animations</font></h3>
<p>Chi Zhou, Zhangjiong Lai, Suzhen Wang, Lincheng Li, Xiaohan Sun, Yu Ding*</p>
<p>Computer Animation and Virtual Worlds, 2021. </p>
</td></tr></tbody></table>

<table class="imgtable"><tbody><tr><td>
<img src="./img/2021-CVPR-ExpEmb.jpg" alt="" width="200px" height="130px">&nbsp;
</td>
<td>
<h3><font color="Purple">Learning a Facial Expression Embedding Disentangled From Identity</font></h3>
<p>Wei Zhang, Xianpeng Ji, Keyu Chen, Yu Ding*, Changjie Fan</p>
<p>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021</p>
[<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_a_Facial_Expression_Embedding_Disentangled_From_Identity_CVPR_2021_paper.pdf">pdf</a>]
</td></tr></tbody></table>

<table class="imgtable"><tbody><tr><td>
<img src="./img/2021-CVPR-Talking.jpg" alt="" width="200px" height="130px">&nbsp;
</td>
<td>
<h3><font color="Purple">Flow-Guided One-Shot Talking Face Generation With a High-Resolution Audio-Visual Dataset</font></h3>
<p>Zhimeng Zhang, Lincheng Li, Yu Ding*, Changjie Fan</p>
<p>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.</p>
[<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Flow-Guided_One-Shot_Talking_Face_Generation_With_a_High-Resolution_Audio-Visual_Dataset_CVPR_2021_paper.pdf">pdf</a>]
[<a href="https://www.youtube.com/watch?v=uJdBgWYBTww">video</a>]
</td></tr></tbody></table>

<table class="imgtable"><tbody><tr><td>
<img src="./img/2021-AAAI-Talking.jpg" alt="" width="200px" height="130px">&nbsp;
</td>
<td>
<h3><font color="Purple">Write-a-speaker: Text-based Emotional and Rhythmic Talking-head Generation</font></h3>
<p>Lincheng Li, Suzhen Wang, Zhimeng Zhang, Yu Ding*, Yixing Zheng, Xin Yu, Changjie Fan</p>
<p>AAAI Conference on Artificial Intelligence  (AAAI), 2021.</p>
[<a href="https://arxiv.org/pdf/2104.07995v2.pdf">pdf</a>]
[<a href="https://www.youtube.com/watch?v=weHA6LHv-Ew">video</a>]
</td></tr></tbody></table>

<table class="imgtable"><tbody><tr><td>
<img src="./img/2021-ICASSP-SpeechConv.jpg" alt="" width="200px" height="130px">&nbsp;
</td>
<td>
<h3><font color="Purple">One-Shot Voice Conversion Using Star-Gan</font></h3>
<p>Ruobai Wang, Yu Ding*, Lincheng Li, Changjie Fan</p>
<p>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020</p>
[<a href="./pdf/2020-ICASSP-VC">pdf</a>]
</td></tr></tbody></table>

<table class="imgtable"><tbody><tr><td>
<img src="./img/2020-CVPR-ExpRe.jpg" alt="" width="200px" height="130px">&nbsp;
</td>
<td>
<h3><font color="Purple">Freenet: Multi-identity face reenactment</font></h3>
<p>Jiangning Zhang, Xianfang Zeng, Mengmeng Wang, Yusu Pan, Liang Liu, Yong Liu, Yu Ding, Changjie Fan</p>
<p>IEEE/CVF Conference on Computer Vision and Pattern Recognition  (CVPR), 2020 </p>
[<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_FReeNet_Multi-Identity_Face_Reenactment_CVPR_2020_paper.pdf">pdf</a>]
[<a href="https://www.youtube.com/watch?v=e39FxP32yNs">video</a>]
</td></tr></tbody></table>

<table class="imgtable"><tbody><tr><td>
<img src="./img/2019-IVA.jpg" alt="" width="200px" height="130px">&nbsp;
</td>
<td>
<h3><font color="Purple">Text-driven Visual Prosody Generation for Embodied Conversational Agents</font></h3>
<p>Jiali Chen, Yong Liu, Zhimeng Zhang, Changjie Fan, Yu Ding*</p>
<p>ACM International Conference on Intelligent Virtual Agents, 2019 </p>
</td></tr></tbody></table>

</ul>
</td>
</tr>
</tbody>
</table>
</div>
</body>
</html>
