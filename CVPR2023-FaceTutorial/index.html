<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>CVPR'23 Skull Restoration, Facial Reconstruction and Expression</title>

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
  <link href="css/style.css" rel="stylesheet" type="text/css" />
</head>

<body>  

<div class="container">

  <table border="0" align="center">
    <tr>
      <td width="750" align="center" valign="middle"><h3>CVPR 2022 Tutorial on</h3>
      <span class="title">Skull Restoration, Facial Reconstruction and Expression</span></td>
    </tr>
    <tr>
        <td colspan="3" align="center"><h3>8:45am - 12:00pm, June 18, 2023</h3></td>
    </tr>
  </table>
 
</div>

</br>

<div class="container">
  <h2>Tutorial Lecturers</h2>
  <br><br>
    <div>
      <div class="instructor">
          <a href="https://people.tamu.edu/~xinli/" >
        <div class="instructorphoto"><img src="photo/xin-li.jpg"></div>
        <div>Tianbao Yang<br>University of Iowa</div>
        </a>
        
        
      </div>

      <div class="instructor">
        <a href="https://www.xu-lan.com/">
            <div class="instructorphoto"><img src="photo/lan-xu.jpg"></div>
            <div>Yiming Ying<br>University at Albany</div>
        </a>
         
      </div>

      <div class="instructor">
          <a href="https://dingyu.github.io/">
        <div class="instructorphoto"><img src="photo/yu-ding.jpg"></div>
        <div>Mingrui Liu<br>George Mason University</div>
        </a>
       
      </div>

    </div>
    <p></p> 
</div>   

</br>

<div class="container">
  <h2>Overview</h2>
    <div class="overview">
  This tutorial focuses on the problems of reconstructing a 3D model from a fragmented skull or a human face and then generating facial expressions. Faces refer to a specific category of objects with particular patterns of identity and expression that can be leveraged to address general problems of reconstruction and modeling. This tutorial is composed of three parts, including facial reconstruction from skeletal remains, 4D dynamic facial performance with high-quality physically-based textures, and audio-driven talking face generation. We will describe these three parts in more detail as follows.
      
  Face modeling (generation and editing) is a fundamental technique and has broad applications in animation, vision, games, and VR. While recent data-driven face modeling techniques achieved great success, facial geometries are fundamentally governed by its underlying skull and tissue structures. This session covers a forensic task of facial reconstruction from skeletal remains, in which the skull, tissue, and face are all studied together. We will discuss how to model anthropological features, restore fragmented skulls, and reconstruct human faces upon them. Involved feature modeling, correspondence, and constrained face generation and editing techniques are general and can benefit many other computer vision tasks.
      
  In the second part, we will then detail state-of-the-art systems and methods to capture 4D dynamic facial performance, which is the foundation for face modeling and rendering with supervised ground truth data. We will consider the hardware design choices for cameras, sensors, lighting, and describe all the steps needed to obtain dynamic facial geometry
along with high-quality physically-based textures, including pore-level diffuse albedo, specular intensity, and normal maps. We will discuss the two traditional and complementary workhorses for recovering facial performances: multi-view stereo and photometric stereo. We will also show how to combine conventional capture pipelines with neural rendering advances to construct neural facial assets. This section also involves the recent trends in combining medical imaging for capturing physically plausible and biologically correct facial performance.    
  
  In the second part, we will then detail state-of-the-art systems and methods to capture 4D dynamic facial performance, which is the foundation for face modeling and rendering with supervised ground truth data. We will consider the hardware design choices for cameras, sensors, lighting, and describe all the steps needed to obtain dynamic facial geometry
along with high-quality physically-based textures, including pore-level diffuse albedo, specular intensity, and normal maps. We will discuss the two traditional and complementary workhorses for recovering facial performances: multi-view stereo and photometric stereo. We will also show how to combine conventional capture pipelines with neural rendering advances to construct neural facial assets. This section also involves the recent trends in combining medical imaging for capturing physically plausible and biologically correct facial performance.    
  <ul>
    <li></li>
    <li></li>
    <li></li>
  </ul>

    </div>
</div>

</br>

<div class="container">
  <h2>Schedule</h2>
    <div class="schedule">
  <p>&#x2022; Introduction  (motivations, definitions, structure)  (8:45 - 9:00am, Yang) <a href="https://drive.google.com/file/d/14D6Zd0RQsCJISV0DCMKxHl7ahHkOuU2h/view?usp=sharing"> [video]</a> </p> 
  <p>&#x2022; Old-school Methods (Batch and Online Methods) (9:00 - 9:15am, Ying)  <a href="https://drive.google.com/file/d/1GZqjW1k4UMvPWJjQjT8Zz8i0Taxn_VwC/view?usp=sharing">[video]</a> </p>
        <p>&#x2022; AUC Maximization in the Era of Big Data (9:15 - 9:45am, Ying)  <a href="https://drive.google.com/file/d/1GZqjW1k4UMvPWJjQjT8Zz8i0Taxn_VwC/view?usp=sharing">[video]</a> </p>
  <p>&#x2022; AUC Maximization in the Era of AI (9:45 - 10:15am, Liu) <a href="https://drive.google.com/file/d/1TdkWrLzwr1Vu9Eh_9EPcG1XqP4psLnx4/view?usp=sharing">[video]</a> </p>     
        <p>&#x2022; Break (10:15 - 10:30am) </p>
        <p>&#x2022; Partial AUC  Maximization: Old and New  (10:30 - 11:00am, Narasimhan) <a href="https://drive.google.com/file/d/1KXATj0qRK0UrGhaIYfGVCQmB-R2KWOPo/view?usp=sharing">[video]</a> </p>
        <p>&#x2022; AP Maximization: Old and New (11:00 - 11:30am, Yang) <a href="https://drive.google.com/file/d/1Q4zlHJ_llhdBI3xfaPiLjjN1USeLTW_I/view?usp=sharing">[video]</a> </p>
        <p>&#x2022; Library & Applications & Future Work (11:30 - 12:00pm, Yang) <a href="https://drive.google.com/file/d/1Q4zlHJ_llhdBI3xfaPiLjjN1USeLTW_I/view?usp=sharing">[video]</a> </p>
    </div>  
  
</div>




</br>

<div class="containersmall">
  <h2 style='text-align:center;'>Resources</h2>
  <ul>
   <li> <p> Slide for the tutorial. <a href="https://docs.google.com/presentation/d/1cSy4tDuYTnBflVbtuoQykDqBYDPmAVmWMx_VmbOilFg/edit?usp=sharing">[Slide]</p></a></li>
   <li><p>LibAUC: A Deep Leanring Library for X-Risk Optimization. [<a href="https://libauc.org/">Website</a>][<a href="https://github.com/Optimization-AI/LibAUC">Github</a>] </p></li>
   <li><p>AUC Maximization in the Era of Big Data and AI: A Survey. Tianbao Yang, Yiming Ying. [<a href="https://arxiv.org/pdf/2203.15046.pdf">PDF</a>] </p></li> 
   <li>This year, June 19 and 20 marks Juneteenth, a US holiday commemorating the end of slavery in the US, and a holiday of special significance in the US South. We encourage attendees to learn more about Juneteenth and its historical context, and to join the city of New Orleans in celebrating the Juneteenth holiday. You can find out more information about Juneteenth here: <a href="https://cvpr2022.thecvf.com/recognizing-juneteenth">https://cvpr2022.thecvf.com/recognizing-juneteenth </a></li>  
  </ul>
</div>

<p hidden><script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=LdxpRW9fITesfkHHPmLW9UCycw2sC4Hj-cyY6yuXnlw&cl=ffffff&w=a"></script></p>
<!--<p align="center" class="acknowledgement">Last updated: Jan. 6, 2017</p>-->
</body>
</html>
